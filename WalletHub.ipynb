{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor \n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=pd.read_csv(\"./dataset_00_with_header.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64      264\n",
      "float64     41\n",
      "dtype: int64\n",
      "False    264\n",
      "True      41\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print (input.sample(2))\n",
    "# print (input.shape)\n",
    "print (input.dtypes.value_counts())\n",
    "# print (input.dtypes)\n",
    "# print (input.isnull().any())\n",
    "print (input.isnull().any().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y       1.000000\n",
      "x235    0.620394\n",
      "x005    0.575970\n",
      "x236    0.572875\n",
      "x022    0.568884\n",
      "x227    0.537190\n",
      "x249    0.515316\n",
      "x226    0.514055\n",
      "x228    0.512067\n",
      "x046    0.507595\n",
      "x244    0.503321\n",
      "x237    0.499302\n",
      "x225    0.488159\n",
      "x025    0.487376\n",
      "x239    0.487305\n",
      "x002    0.485744\n",
      "x023    0.480639\n",
      "x014    0.474342\n",
      "x245    0.467735\n",
      "x246    0.459512\n",
      "x250    0.447270\n",
      "x247    0.446191\n",
      "x262    0.445821\n",
      "x229    0.443279\n",
      "x261    0.439065\n",
      "x260    0.434920\n",
      "x224    0.432531\n",
      "x030    0.425470\n",
      "x027    0.424206\n",
      "x004    0.419683\n",
      "          ...   \n",
      "x276   -0.307428\n",
      "x099   -0.307428\n",
      "x277   -0.308842\n",
      "x172   -0.309105\n",
      "x036   -0.312055\n",
      "x278   -0.316596\n",
      "x056   -0.336567\n",
      "x173   -0.337363\n",
      "x168   -0.348083\n",
      "x063   -0.358450\n",
      "x304   -0.368138\n",
      "x293   -0.370417\n",
      "x297   -0.374772\n",
      "x162   -0.403556\n",
      "x064   -0.411101\n",
      "x065   -0.413885\n",
      "x059   -0.415029\n",
      "x253   -0.473823\n",
      "x148   -0.483754\n",
      "x302   -0.483852\n",
      "x155   -0.542090\n",
      "x242   -0.547844\n",
      "x287   -0.562191\n",
      "x058   -0.586811\n",
      "x057   -0.636510\n",
      "x041   -0.690840\n",
      "x067         NaN\n",
      "x094         NaN\n",
      "x095         NaN\n",
      "x096         NaN\n",
      "Name: y, Length: 305, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#calculating the corelation\n",
    "corr_matrix=input.corr(method ='pearson')#['y'][:]\n",
    "print(corr_matrix[\"y\"].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    100000\n",
      "Name: x067, dtype: int64\n",
      "0    100000\n",
      "Name: x094, dtype: int64\n",
      "0    100000\n",
      "Name: x095, dtype: int64\n",
      "0    100000\n",
      "Name: x096, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# because suspicious corelation find the values in the column\n",
    "print (input[\"x067\"].value_counts())\n",
    "print (input[\"x094\"].value_counts())\n",
    "print (input[\"x095\"].value_counts())\n",
    "print (input[\"x096\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with more than 75% null values\n",
    "percent_missing = input.isnull().sum() * 100 / len(input)\n",
    "missing_value_df = pd.DataFrame({'column_name': input.columns,\n",
    "                                 'percent_missing': percent_missing})\n",
    "\n",
    "missing_value_df.sort_values('percent_missing', inplace=True)\n",
    "nullDeleteList=list(missing_value_df.loc[missing_value_df[\"percent_missing\"]>75][\"column_name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all the features in removal list \n",
    "\n",
    "deleteCell=['x067','x094','x095','x096']\n",
    "# nullDeleteList+deleteCell\n",
    "\n",
    "nullDeleteList.extend(deleteCell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [c for c in input.columns if c not in nullDeleteList]\n",
    "numclasses=[]\n",
    "for c in col:\n",
    "    numclasses.append(len(np.unique(input[[c]])))\n",
    "\n",
    "#create categorical features which has only 2-3 values in \n",
    "threshold=3\n",
    "categorical_variables = list(np.array(col)[np.array(numclasses)<=threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    41\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all removal columns\n",
    "input1=input[col]\n",
    "# find the nunll values in categorical feature if any\n",
    "len(categorical_variables)\n",
    "input1[categorical_variables].isnull().any().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train test \n",
    "X_train, X_test, train_y, test_y = train_test_split(input1.drop(['y'],1), input1.y , test_size=0.3)\n",
    "X_test, X_val, test_y, val_y = train_test_split(X_test, test_y , test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = X_train[categorical_variables]\n",
    "test_cat = X_test[categorical_variables]\n",
    "val_cat=X_val[categorical_variables]\n",
    "# Do the one Hot encoding of the categorical features\n",
    "\n",
    "train_cat_encoded = pd.get_dummies(train_cat, columns = train_cat.columns.tolist())\n",
    "test_cat_encoded = pd.get_dummies(test_cat, columns = test_cat.columns.tolist())\n",
    "val_cat_encoded = pd.get_dummies(val_cat, columns = val_cat.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the ids as unique identifier from the contious feature set\n",
    "\n",
    "train_cont = X_train.drop(categorical_variables,1)\n",
    "test_cont = X_test.drop(categorical_variables,1)\n",
    "val_cont = X_val.drop(categorical_variables,1)\n",
    "\n",
    "X_train.drop(['x001'], 1,inplace=True)\n",
    "X_test.drop(['x001'], 1,inplace=True)\n",
    "X_val.drop(['x001'], 1,inplace=True)\n",
    "\n",
    "train_cont.fillna(train_cont.mean(), inplace=True)\n",
    "test_cont.fillna(test_cont.mean(), inplace=True)\n",
    "val_cont.fillna(val_cont.mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x001</th>\n",
       "      <th>x002</th>\n",
       "      <th>x003</th>\n",
       "      <th>x004</th>\n",
       "      <th>x005</th>\n",
       "      <th>x007</th>\n",
       "      <th>x008</th>\n",
       "      <th>x009</th>\n",
       "      <th>x010</th>\n",
       "      <th>x011</th>\n",
       "      <th>...</th>\n",
       "      <th>x289</th>\n",
       "      <th>x290</th>\n",
       "      <th>x291</th>\n",
       "      <th>x292</th>\n",
       "      <th>x293</th>\n",
       "      <th>x294</th>\n",
       "      <th>x296</th>\n",
       "      <th>x297</th>\n",
       "      <th>x302</th>\n",
       "      <th>x303</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76483</th>\n",
       "      <td>0.774600</td>\n",
       "      <td>1.376446</td>\n",
       "      <td>-0.057928</td>\n",
       "      <td>0.432320</td>\n",
       "      <td>0.737089</td>\n",
       "      <td>-0.496204</td>\n",
       "      <td>-0.599437</td>\n",
       "      <td>-0.094936</td>\n",
       "      <td>-0.016809</td>\n",
       "      <td>0.056668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015118</td>\n",
       "      <td>-0.017308</td>\n",
       "      <td>-0.380129</td>\n",
       "      <td>0.104372</td>\n",
       "      <td>0.013534</td>\n",
       "      <td>-0.148742</td>\n",
       "      <td>-0.369220</td>\n",
       "      <td>0.016290</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>-0.265911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47554</th>\n",
       "      <td>0.429904</td>\n",
       "      <td>-0.031094</td>\n",
       "      <td>-0.007722</td>\n",
       "      <td>-0.024532</td>\n",
       "      <td>-0.666091</td>\n",
       "      <td>-0.496204</td>\n",
       "      <td>-0.599437</td>\n",
       "      <td>-0.589502</td>\n",
       "      <td>0.563374</td>\n",
       "      <td>-0.679617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015118</td>\n",
       "      <td>-0.017308</td>\n",
       "      <td>-0.380129</td>\n",
       "      <td>-0.350807</td>\n",
       "      <td>0.013534</td>\n",
       "      <td>-0.148742</td>\n",
       "      <td>-0.369220</td>\n",
       "      <td>0.016290</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>-0.265911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28147</th>\n",
       "      <td>1.513359</td>\n",
       "      <td>-0.031094</td>\n",
       "      <td>-0.007722</td>\n",
       "      <td>-0.024532</td>\n",
       "      <td>-0.583551</td>\n",
       "      <td>-0.496204</td>\n",
       "      <td>-0.599437</td>\n",
       "      <td>-0.589502</td>\n",
       "      <td>-0.596992</td>\n",
       "      <td>-0.679617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015118</td>\n",
       "      <td>-0.017308</td>\n",
       "      <td>-0.380129</td>\n",
       "      <td>-0.350807</td>\n",
       "      <td>0.013534</td>\n",
       "      <td>-0.148742</td>\n",
       "      <td>-0.369220</td>\n",
       "      <td>0.016290</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>-0.265911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46327</th>\n",
       "      <td>1.329072</td>\n",
       "      <td>0.329569</td>\n",
       "      <td>3.088769</td>\n",
       "      <td>1.668051</td>\n",
       "      <td>-0.154343</td>\n",
       "      <td>-0.496204</td>\n",
       "      <td>-0.599437</td>\n",
       "      <td>-0.589502</td>\n",
       "      <td>-0.596992</td>\n",
       "      <td>-0.679617</td>\n",
       "      <td>...</td>\n",
       "      <td>5.423473</td>\n",
       "      <td>4.385160</td>\n",
       "      <td>-0.376181</td>\n",
       "      <td>-0.349898</td>\n",
       "      <td>-3.490871</td>\n",
       "      <td>-0.148742</td>\n",
       "      <td>-0.364460</td>\n",
       "      <td>-4.124685</td>\n",
       "      <td>-1.763056</td>\n",
       "      <td>-0.259598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31200</th>\n",
       "      <td>-2.728080</td>\n",
       "      <td>-0.862707</td>\n",
       "      <td>-0.563237</td>\n",
       "      <td>-0.891677</td>\n",
       "      <td>-0.699107</td>\n",
       "      <td>1.641466</td>\n",
       "      <td>8.434803</td>\n",
       "      <td>39.470413</td>\n",
       "      <td>4.044474</td>\n",
       "      <td>-0.311475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300862</td>\n",
       "      <td>-0.135015</td>\n",
       "      <td>1.577190</td>\n",
       "      <td>0.100034</td>\n",
       "      <td>1.237362</td>\n",
       "      <td>-0.148742</td>\n",
       "      <td>1.990378</td>\n",
       "      <td>1.594966</td>\n",
       "      <td>1.855582</td>\n",
       "      <td>2.864152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x001      x002      x003      x004      x005      x007      x008  \\\n",
       "76483  0.774600  1.376446 -0.057928  0.432320  0.737089 -0.496204 -0.599437   \n",
       "47554  0.429904 -0.031094 -0.007722 -0.024532 -0.666091 -0.496204 -0.599437   \n",
       "28147  1.513359 -0.031094 -0.007722 -0.024532 -0.583551 -0.496204 -0.599437   \n",
       "46327  1.329072  0.329569  3.088769  1.668051 -0.154343 -0.496204 -0.599437   \n",
       "31200 -2.728080 -0.862707 -0.563237 -0.891677 -0.699107  1.641466  8.434803   \n",
       "\n",
       "            x009      x010      x011    ...         x289      x290      x291  \\\n",
       "76483  -0.094936 -0.016809  0.056668    ...    -0.015118 -0.017308 -0.380129   \n",
       "47554  -0.589502  0.563374 -0.679617    ...    -0.015118 -0.017308 -0.380129   \n",
       "28147  -0.589502 -0.596992 -0.679617    ...    -0.015118 -0.017308 -0.380129   \n",
       "46327  -0.589502 -0.596992 -0.679617    ...     5.423473  4.385160 -0.376181   \n",
       "31200  39.470413  4.044474 -0.311475    ...     0.300862 -0.135015  1.577190   \n",
       "\n",
       "           x292      x293      x294      x296      x297      x302      x303  \n",
       "76483  0.104372  0.013534 -0.148742 -0.369220  0.016290  0.000254 -0.265911  \n",
       "47554 -0.350807  0.013534 -0.148742 -0.369220  0.016290  0.000254 -0.265911  \n",
       "28147 -0.350807  0.013534 -0.148742 -0.369220  0.016290  0.000254 -0.265911  \n",
       "46327 -0.349898 -3.490871 -0.148742 -0.364460 -4.124685 -1.763056 -0.259598  \n",
       "31200  0.100034  1.237362 -0.148742  1.990378  1.594966  1.855582  2.864152  \n",
       "\n",
       "[5 rows x 250 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do normalization of the feature set and learn that norm values(mean and std dev) \n",
    "#                                     on train set and apply it on test and train\n",
    "\n",
    "norm_values= train_cont.agg([\"mean\",\"std\"])\n",
    "\n",
    "train_cont_normalized = train_cont.apply(lambda x: (x- x.mean())/x.std())\n",
    "train_cont_normalized.head()\n",
    "# pickle.dump(norm_values, open(\"/data/analytics/norm_values.pkl\", 'wb'))\n",
    "######## normailising using train normalisers \n",
    "test_cont_normalized = test_cont - norm_values.loc[\"mean\"]\n",
    "test_cont_normalized = test_cont_normalized/norm_values.loc[\"std\"]\n",
    "test_cont_normalized.head()\n",
    "\n",
    "######## normailising using train normalisers \n",
    "val_cont_normalized = val_cont - norm_values.loc[\"mean\"]\n",
    "val_cont_normalized = val_cont_normalized/norm_values.loc[\"std\"]\n",
    "val_cont_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the both cat and cont features\n",
    "train_X= pd.concat([train_cat_encoded,train_cont_normalized],axis =1)\n",
    "test_X = pd.concat([test_cat_encoded,test_cont_normalized],axis =1)\n",
    "val_X = pd.concat([val_cat_encoded,val_cont_normalized],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any ferature from training set missing in test set and remove them\n",
    "columns=train_X.columns\n",
    "missing_cols=set(columns)-set( test_X.columns )     \n",
    "feature_difference_df = pd.DataFrame(data=np.zeros((test_X.shape[0], len(missing_cols))),\n",
    "                                 columns=list(missing_cols))\n",
    "\n",
    "# add \"missing\" features back to `test\n",
    "test_X = test_X.join(feature_difference_df)\n",
    "test_X=test_X[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    332\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check any null values left \n",
    "print(test_X.isnull().any().value_counts())\n",
    "test_X.fillna(0, inplace=True)\n",
    "train_X.fillna(0, inplace=True)\n",
    "val_X.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---XgBoost-MSE-- 25.071598959593953\n",
      "---XgBoost-R2-- 0.955163125418\n"
     ]
    }
   ],
   "source": [
    "############ XGBoost##############\n",
    "xgb= XGBRegressor(seed=12,\n",
    "            n_estimators=1000, max_depth=10,nthread =55,learning_rate=0.03001, subsample=0.65, colsample_bytree=0.6\n",
    "           ,min_child_weight=1, max_delta_step=0, \n",
    "                  colsample_bylevel=1, objective=\"reg:tweedie\",\n",
    "                 reg_alpha=0.8, reg_lambda=1, base_score=0.5,  missing=None\n",
    "        )\n",
    "\n",
    "xgb.fit(train_X, train_y)\n",
    "prediction_xgbp=xgb.predict(test_X)\n",
    "print(\"---XgBoost-MSE--\", sqrt(mean_squared_error(test_y, prediction_xgbp))  )\n",
    "print(\"---XgBoost-R2--\", xgb.score(test_X,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Logistic-MSE-- 47.67096097397547\n",
      "---Logistic-R2-- 0.837901468394\n"
     ]
    }
   ],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.fit(train_X, train_y)#predicting the test set results\n",
    "y_pred = regressor.predict(test_X)\n",
    "\n",
    "print(\"---Logistic-MSE--\", sqrt(mean_squared_error(test_y, y_pred))  )\n",
    "print(\"---Logistic-R2--\", regressor.score(test_X,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.9326783763\n",
      "---gbr-MSE-- 26.548938672922223\n",
      "---gbr-R2-- 0.949723435259\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scoring(clf):\n",
    "    scores = cross_val_score(clf, train_X, train_y, cv=15, n_jobs=55, scoring = 'neg_median_absolute_error')\n",
    "    print (np.median(scores) * -1)\n",
    "gbr = GradientBoostingRegressor(learning_rate = 0.12,\n",
    "                                n_estimators = 150,\n",
    "                                max_depth = 8,\n",
    "                                min_samples_leaf = 1,\n",
    "                                random_state = 2)\n",
    "# clf = GradientBoostingRegressor(n_estimators = 400, max_depth = 5, min_samples_split = 2,\n",
    "#           learning_rate = 0.1, loss = 'ls')\n",
    "scoring(gbr)\n",
    "\n",
    "gbr.fit(train_X, train_y)\n",
    "prediction_gbr=gbr.predict(test_X)\n",
    "print(\"---gbr-MSE--\", sqrt(mean_squared_error(test_y, prediction_gbr))  )\n",
    "print(\"---gbr-R2--\", gbr.score(test_X,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Neural-MSE-- 28.88725433440957\n",
      "---Neural-R2-- 0.940477139013\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# clfN2 = MLPRegressor(solver='adam', alpha=.001,activation =\"relu\",verbose=True,\n",
    "#                     hidden_layer_sizes=(128,64,32,16,8), random_state=2,max_iter=5000,learning_rate_init=0.0005)\n",
    "\n",
    "\n",
    "clfN2 = MLPRegressor(solver='adam', alpha=.001,activation =\"relu\",\n",
    "                    hidden_layer_sizes=(128,64,32,16), random_state=2,max_iter=5000)\n",
    "clfN2.fit(train_X, train_y)\n",
    "prediction_neuralNet2 = clfN2.predict(test_X)\n",
    "print(\"---Neural-MSE--\",sqrt(mean_squared_error(test_y, prediction_neuralNet2)))\n",
    "print(\"---Neural-R2--\", clfN2.score(test_X,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.7536626\n",
      "---rfr-MSE-- 29.021922675037537\n",
      "---rfr-R2-- 0.939920870895\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(n_estimators = 55,\n",
    "                            min_samples_leaf = 3,\n",
    "                            random_state = 2)\n",
    "scoring(rfr)\n",
    "\n",
    "rfr.fit(train_X, train_y)\n",
    "prediction_rfr=rfr.predict(test_X)\n",
    "print(\"---rfr-MSE--\", sqrt(mean_squared_error(test_y, prediction_rfr))  )\n",
    "print(\"---rfr-R2--\", rfr.score(test_X,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kfold = KFold(n_splits=4, random_state=7)\n",
    "results = cross_val_score(xgb, val_X, val_y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9391913 ,  0.94386338,  0.93509151,  0.94596736])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############ Tried adding polynomial features\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# poly = PolynomialFeatures(2)\n",
    "# train_poly=poly.fit_transform(train_X)\n",
    "# test_poly=poly.fit_transform(test_X)\n",
    "\n",
    "# regressor = LinearRegression()\n",
    "# regressor.fit(train_poly, train_y)#predicting the test set results\n",
    "\n",
    "# y_pred = regressor.predict(test_poly)\n",
    "\n",
    "# print(\"---Logistic-MSE--\", mean_squared_error(test_y, y_pred))  \n",
    "# print(\"---Logistic-R2--\", regressor.score(test_poly,test_y))\n",
    "\n",
    "\n",
    "# ---Logistic-MSE-- 7.99538721465e+22\n",
    "# ---Logistic-R2-- -5.68078419922e+18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# joblib.dump(xgb, 'model.pkl')\n",
    "pickle.dump(xgb, open(\"/data/analytics/anuj/model.pkl\", 'wb'))\n",
    "# gbr = joblib.load('model.pkl')\n",
    "\n",
    "pickle.dump(norm_values, open(\"/data/analytics/anuj/norm_values.pkl\", 'wb'))\n",
    "pickle.dump(columns, open(\"/data/analytics/anuj/columnsTrain.pkl\", 'wb'))\n",
    "pickle.dump(col, open(\"/data/analytics/anuj/genuineCol.pkl\", 'wb'))\n",
    "pickle.dump(categorical_variables, open(\"//data/analytics/anuj/catvariables.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Accuracy -- 15.3\n"
     ]
    }
   ],
   "source": [
    "otpt=pd.DataFrame(test_y.values, prediction_xgbp).reset_index()\n",
    "otpt.columns =['actual','pred']\n",
    "otpt[\"diff\"]=abs(otpt[\"actual\"]-otpt[\"pred\"])\n",
    "otpt[\"class\"]=0\n",
    "otpt.loc[otpt[\"diff\"]<=3.0,\"class\"]=1\n",
    "final=otpt[\"class\"].value_counts()*100/otpt.shape[0] \n",
    "print(\"---Accuracy --\", final[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a plot that ranks the features by importance.\n",
    "def plot_importances(model, model_name):\n",
    "    importances = model.feature_importances_\n",
    "    std = np.std([model.feature_importances_ for feature in model.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]    \n",
    "\n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure(figsize = (8,5))\n",
    "    plt.title(\"Feature importances of \" + model_name)\n",
    "    plt.bar(range(X_train.shape[1]), importances[indices], color=\"r\", align=\"center\")\n",
    "    plt.xticks(range(X_train.shape[1]), indices)\n",
    "    plt.xlim([-1, X_train.shape[1]])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "0 x006_0\n",
      "1 x006_1\n",
      "2 x025_0\n",
      "3 x025_1\n",
      "4 x026_0\n",
      "5 x026_1\n",
      "6 x027_0\n",
      "7 x027_1\n",
      "8 x060_0\n",
      "9 x060_1\n",
      "10 x082_0\n",
      "11 x082_1\n",
      "12 x083_0\n",
      "13 x083_1\n",
      "14 x084_0\n",
      "15 x084_1\n",
      "16 x085_0\n",
      "17 x085_1\n",
      "18 x086_0\n",
      "19 x086_1\n",
      "20 x087_0\n",
      "21 x087_1\n",
      "22 x088_0\n",
      "23 x088_1\n",
      "24 x089_0\n",
      "25 x089_1\n",
      "26 x090_0\n",
      "27 x090_1\n",
      "28 x091_0\n",
      "29 x091_1\n",
      "30 x092_0\n",
      "31 x092_1\n",
      "32 x093_0\n",
      "33 x093_1\n",
      "34 x147_0\n",
      "35 x147_1\n",
      "36 x154_0\n",
      "37 x154_1\n",
      "38 x161_0\n",
      "39 x161_1\n",
      "40 x180_0\n",
      "41 x180_1\n",
      "42 x244_0\n",
      "43 x244_1\n",
      "44 x245_0\n",
      "45 x245_1\n",
      "46 x246_0\n",
      "47 x246_1\n",
      "48 x247_0\n",
      "49 x247_1\n",
      "50 x248_0\n",
      "51 x248_1\n",
      "52 x249_0\n",
      "53 x249_1\n",
      "54 x260_0\n",
      "55 x260_1\n",
      "56 x261_0\n",
      "57 x261_1\n",
      "58 x262_0\n",
      "59 x262_1\n",
      "60 x263_0\n",
      "61 x263_1\n",
      "62 x269_0\n",
      "63 x269_1\n",
      "64 x270_0\n",
      "65 x270_1\n",
      "66 x271_0\n",
      "67 x271_1\n",
      "68 x282_0\n",
      "69 x282_1\n",
      "70 x283_0\n",
      "71 x283_1\n",
      "72 x284_0\n",
      "73 x284_1\n",
      "74 x298_0\n",
      "75 x298_1\n",
      "76 x299_0\n",
      "77 x299_1\n",
      "78 x300_0\n",
      "79 x300_1\n",
      "80 x301_0\n",
      "81 x301_1\n",
      "82 x001\n",
      "83 x002\n",
      "84 x003\n",
      "85 x004\n",
      "86 x005\n",
      "87 x007\n",
      "88 x008\n",
      "89 x009\n",
      "90 x010\n",
      "91 x011\n",
      "92 x012\n",
      "93 x013\n",
      "94 x014\n",
      "95 x015\n",
      "96 x016\n",
      "97 x017\n",
      "98 x018\n",
      "99 x019\n",
      "100 x020\n",
      "101 x021\n",
      "102 x022\n",
      "103 x023\n",
      "104 x024\n",
      "105 x028\n",
      "106 x029\n",
      "107 x030\n",
      "108 x031\n",
      "109 x032\n",
      "110 x033\n",
      "111 x034\n",
      "112 x035\n",
      "113 x036\n",
      "114 x037\n",
      "115 x038\n",
      "116 x039\n",
      "117 x040\n",
      "118 x041\n",
      "119 x042\n",
      "120 x043\n",
      "121 x044\n",
      "122 x045\n",
      "123 x046\n",
      "124 x047\n",
      "125 x048\n",
      "126 x049\n",
      "127 x050\n",
      "128 x051\n",
      "129 x052\n",
      "130 x053\n",
      "131 x054\n",
      "132 x055\n",
      "133 x056\n",
      "134 x057\n",
      "135 x058\n",
      "136 x059\n",
      "137 x061\n",
      "138 x062\n",
      "139 x063\n",
      "140 x064\n",
      "141 x065\n",
      "142 x066\n",
      "143 x068\n",
      "144 x069\n",
      "145 x070\n",
      "146 x071\n",
      "147 x072\n",
      "148 x073\n",
      "149 x074\n",
      "150 x075\n",
      "151 x076\n",
      "152 x077\n",
      "153 x078\n",
      "154 x079\n",
      "155 x080\n",
      "156 x081\n",
      "157 x097\n",
      "158 x099\n",
      "159 x100\n",
      "160 x101\n",
      "161 x102\n",
      "162 x103\n",
      "163 x104\n",
      "164 x105\n",
      "165 x106\n",
      "166 x107\n",
      "167 x108\n",
      "168 x109\n",
      "169 x110\n",
      "170 x111\n",
      "171 x112\n",
      "172 x113\n",
      "173 x114\n",
      "174 x115\n",
      "175 x116\n",
      "176 x117\n",
      "177 x118\n",
      "178 x119\n",
      "179 x120\n",
      "180 x121\n",
      "181 x122\n",
      "182 x123\n",
      "183 x124\n",
      "184 x125\n",
      "185 x126\n",
      "186 x127\n",
      "187 x128\n",
      "188 x129\n",
      "189 x130\n",
      "190 x131\n",
      "191 x132\n",
      "192 x133\n",
      "193 x134\n",
      "194 x135\n",
      "195 x136\n",
      "196 x137\n",
      "197 x138\n",
      "198 x139\n",
      "199 x140\n",
      "200 x141\n",
      "201 x142\n",
      "202 x143\n",
      "203 x144\n",
      "204 x145\n",
      "205 x146\n",
      "206 x148\n",
      "207 x149\n",
      "208 x150\n",
      "209 x151\n",
      "210 x152\n",
      "211 x153\n",
      "212 x156\n",
      "213 x157\n",
      "214 x158\n",
      "215 x159\n",
      "216 x160\n",
      "217 x162\n",
      "218 x163\n",
      "219 x164\n",
      "220 x165\n",
      "221 x166\n",
      "222 x167\n",
      "223 x168\n",
      "224 x169\n",
      "225 x170\n",
      "226 x171\n",
      "227 x172\n",
      "228 x173\n",
      "229 x174\n",
      "230 x175\n",
      "231 x176\n",
      "232 x177\n",
      "233 x178\n",
      "234 x179\n",
      "235 x181\n",
      "236 x182\n",
      "237 x183\n",
      "238 x184\n",
      "239 x185\n",
      "240 x186\n",
      "241 x187\n",
      "242 x188\n",
      "243 x189\n",
      "244 x190\n",
      "245 x191\n",
      "246 x192\n",
      "247 x193\n",
      "248 x194\n",
      "249 x195\n",
      "250 x196\n",
      "251 x197\n",
      "252 x198\n",
      "253 x199\n",
      "254 x200\n",
      "255 x201\n",
      "256 x202\n",
      "257 x203\n",
      "258 x204\n",
      "259 x205\n",
      "260 x206\n",
      "261 x207\n",
      "262 x208\n",
      "263 x209\n",
      "264 x210\n",
      "265 x211\n",
      "266 x212\n",
      "267 x213\n",
      "268 x214\n",
      "269 x215\n",
      "270 x216\n",
      "271 x217\n",
      "272 x218\n",
      "273 x219\n",
      "274 x220\n",
      "275 x221\n",
      "276 x222\n",
      "277 x223\n",
      "278 x224\n",
      "279 x225\n",
      "280 x226\n",
      "281 x227\n",
      "282 x228\n",
      "283 x229\n",
      "284 x230\n",
      "285 x231\n",
      "286 x232\n",
      "287 x233\n",
      "288 x234\n",
      "289 x235\n",
      "290 x236\n",
      "291 x237\n",
      "292 x238\n",
      "293 x239\n",
      "294 x240\n",
      "295 x241\n",
      "296 x243\n",
      "297 x250\n",
      "298 x251\n",
      "299 x252\n",
      "300 x253\n",
      "301 x254\n",
      "302 x258\n",
      "303 x264\n",
      "304 x265\n",
      "305 x266\n",
      "306 x267\n",
      "307 x268\n",
      "308 x272\n",
      "309 x273\n",
      "310 x274\n",
      "311 x275\n",
      "312 x276\n",
      "313 x277\n",
      "314 x278\n",
      "315 x279\n",
      "316 x280\n",
      "317 x281\n",
      "318 x285\n",
      "319 x286\n",
      "320 x287\n",
      "321 x288\n",
      "322 x289\n",
      "323 x290\n",
      "324 x291\n",
      "325 x292\n",
      "326 x293\n",
      "327 x294\n",
      "328 x296\n",
      "329 x297\n",
      "330 x302\n",
      "331 x303\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature ranking:\")\n",
    "\n",
    "i = 0\n",
    "for feature in train_X:\n",
    "    print (i, feature)\n",
    "    i += 1\n",
    "    \n",
    "\n",
    "# plot_importances(gbr, \"Gradient Boosting Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
